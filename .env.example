# Q/A Orchestrator configuration

# Hybrid backend mode (local -> docker fallback)
RAG_ENGINE_LOCAL_URL=http://localhost:8000
RAG_ENGINE_DOCKER_URL=http://api:8000
RAG_ENGINE_HEALTH_PATH=/health
RAG_ENGINE_PROBE_TIMEOUT_MS=300
RAG_ENGINE_BACKEND_TTL_SECONDS=20
# Optional: local|docker
# RAG_ENGINE_FORCE_BACKEND=

# Ingestion client wrapper (optional override)
# RAG_URL=http://localhost:8000

# API runtime
LOG_LEVEL=INFO
ORCH_LOG_EXC_INFO=false
ORCHESTRATOR_PORT=8001
ORCH_AUTH_REQUIRED=true
ORCH_DEV_TENANT_CREATE_ENABLED=false
RAG_SERVICE_SECRET=

# Supabase Auth / Membership
SUPABASE_URL=
SUPABASE_ANON_KEY=
# Optional override. If empty, uses ${SUPABASE_URL}/auth/v1/.well-known/jwks.json
SUPABASE_JWKS_URL=
SUPABASE_JWT_AUDIENCE=authenticated
SUPABASE_SERVICE_ROLE_KEY=
SUPABASE_MEMBERSHIPS_TABLE=tenant_memberships
SUPABASE_MEMBERSHIP_USER_COLUMN=user_id
SUPABASE_MEMBERSHIP_TENANT_COLUMN=tenant_id

# Optional answer-generation providers
GEMINI_API_KEY=
GEMINI_MODEL_NAME=gemini-2.5-flash-lite
GEMINI_FLASH=gemini-2.5-flash-lite

GROQ_API_KEY=
GROQ_MODEL_LIGHTWEIGHT=openai/gpt-oss-20b
GROQ_MODEL_HEAVY=openai/gpt-oss-120b
GROQ_MODEL_DESIGN=openai/gpt-oss-120b
GROQ_MODEL_CHAT=openai/gpt-oss-20b
GROQ_MODEL_FORENSIC=openai/gpt-oss-120b
GROQ_MODEL_ORCHESTRATION=openai/gpt-oss-20b
GROQ_MODEL_SUMMARIZATION=openai/gpt-oss-20b

# Validation tuning
QA_LITERAL_SEMANTIC_FALLBACK_ENABLED=true
QA_LITERAL_SEMANTIC_MIN_KEYWORD_OVERLAP=2
QA_LITERAL_SEMANTIC_MIN_SIMILARITY=0.3

# Retrieval kernel toggles
ORCH_DETERMINISTIC_SUBQUERY_SEMANTIC_TAIL=true
ORCH_MIN_SCORE_BACKSTOP_ENABLED=true
ORCH_MIN_SCORE_BACKSTOP_TOP_N=6
ORCH_MULTI_QUERY_CLIENT_FANOUT_ENABLED=false
ORCH_MULTI_QUERY_CLIENT_FANOUT_MAX_PARALLEL=2
ORCH_MULTI_QUERY_CLIENT_FANOUT_PER_QUERY_TIMEOUT_MS=8000
ORCH_MULTI_QUERY_CLIENT_FANOUT_RERANK_ENABLED=false
ORCH_MULTI_QUERY_CLIENT_FANOUT_GRAPH_MAX_HOPS=2
