# Q/A Orchestrator configuration

# Hybrid backend mode (local -> docker fallback)
RAG_ENGINE_LOCAL_URL=http://localhost:8000
RAG_ENGINE_DOCKER_URL=http://api:8000
RAG_ENGINE_HEALTH_PATH=/health
RAG_ENGINE_PROBE_TIMEOUT_MS=300
RAG_ENGINE_BACKEND_TTL_SECONDS=20
# Optional: local|docker
# RAG_ENGINE_FORCE_BACKEND=

# Ingestion client wrapper (optional override)
# RAG_URL=http://localhost:8000

# API runtime
LOG_LEVEL=INFO
ORCHESTRATOR_PORT=8001
ORCH_AUTH_REQUIRED=true
RAG_SERVICE_SECRET=

# Supabase Auth / Membership
SUPABASE_URL=
SUPABASE_ANON_KEY=
# Optional override. If empty, uses ${SUPABASE_URL}/auth/v1/.well-known/jwks.json
SUPABASE_JWKS_URL=
SUPABASE_JWT_AUDIENCE=authenticated
SUPABASE_SERVICE_ROLE_KEY=
SUPABASE_MEMBERSHIPS_TABLE=tenant_memberships
SUPABASE_MEMBERSHIP_USER_COLUMN=user_id
SUPABASE_MEMBERSHIP_TENANT_COLUMN=tenant_id

# Optional answer-generation providers
GEMINI_API_KEY=
GEMINI_MODEL_NAME=gemini-2.5-flash-lite
GEMINI_FLASH=gemini-2.5-flash-lite

GROQ_API_KEY=
GROQ_MODEL_LIGHTWEIGHT=openai/gpt-oss-20b
GROQ_MODEL_HEAVY=llama-3.3-70b-versatile
GROQ_MODEL_DESIGN=llama-3.3-70b-versatile
GROQ_MODEL_CHAT=openai/gpt-oss-20b
GROQ_MODEL_FORENSIC=llama-3.3-70b-versatile
GROQ_MODEL_ORCHESTRATION=openai/gpt-oss-20b
GROQ_MODEL_SUMMARIZATION=openai/gpt-oss-20b

# Validation tuning
QA_LITERAL_SEMANTIC_FALLBACK_ENABLED=true
QA_LITERAL_SEMANTIC_MIN_KEYWORD_OVERLAP=2
QA_LITERAL_SEMANTIC_MIN_SIMILARITY=0.3
